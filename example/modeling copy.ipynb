{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r'C:\\Users\\carlo\\GitHub\\ProjectBuffalo')\n",
    "\n",
    "import buffalo.ingestion as ingestion\n",
    "import buffalo.predictor as predictor\n",
    "import buffalo.algorithm as algorithm\n",
    "import buffalo.predictor.models as modeling\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from buffalo.utility import expand_grid, do_call_for_each_group\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor = ingestion.DataIngestion(ingestion.enum.API.ADVANTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor.load_data(r'cached_data/ingestion.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stock = ingestor.data['ADJUSTED_DAILY_STOCK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_income_statement = ingestor.data['COMPANY_INCOME_STATEMENT'].query('freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function']).dropna(axis=1, how='all')\n",
    "target_balance_sheet = ingestor.data['COMPANY_BALANCE_SHEET'].query('freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function']).dropna(axis=1, how='all')\n",
    "target_cash_flow = ingestor.data['COMPANY_CASH_FLOW'].query('freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function', 'net_income']).dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_funds_rate = ingestor.data['FEDERAL_FUNDS_RATE'][['value']].rename(columns={'value': 'effective_federal_funds_rate'}).dropna(axis=1, how='all')\n",
    "payroll = ingestor.data['NONFARM_PAYROLL'][['value']].rename(columns={'value': 'total_nonfarm_payroll'}).dropna(axis=1, how='all')\n",
    "cpi = ingestor.data['CPI'][['value']].rename(columns={'value': 'consumer_price_index'}).dropna(axis=1, how='all')\n",
    "unemployment = ingestor.data['UNEMPLOYMENT'][['value']].rename(columns={'value': 'unemployment_rate'}).dropna(axis=1, how='all')\n",
    "real_gdp = ingestor.data['REAL_GDP'][['value']].rename(columns={'value': 'real_gross_domestic_product'}).dropna(axis=1, how='all')\n",
    "real_gdp_per_capita = ingestor.data['REAL_GDP_PER_CAPITA'][['value']].rename(columns={'value': 'real_gross_domestic_product_per_capita'})\n",
    "treasury_yield = ingestor.data['TREASURY_YIELD'][['value', 'maturity']].pivot(columns=['maturity'], values=['value']).dropna(axis=1, how='all')\n",
    "treasury_yield.columns = 'treasury_yield_' + treasury_yield.columns.droplevel(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_symbols = ['MSFT', 'IBM', 'JNJ', 'PFE', 'UNH', 'XLV', 'JPM', 'BAC', 'GS', 'XLF', 'AAPL', 'KO', 'PEP', 'NKE', 'XLP', 'PG', 'HON', 'MMM', 'XLI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stock = target_stock[target_stock['symbol'].isin(filtered_symbols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = []\n",
    "for symb, df in target_stock.groupby('symbol'):\n",
    "    symb = df.iloc[0]['symbol']\n",
    "    df = df[['open', 'high', 'low', 'adjusted_close', 'volume']].rename(columns={'adjusted_close': 'close'})\n",
    "    df.columns = symb + '_' + df.columns\n",
    "    stocks.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = reduce(predictor.util.align_dataframe_by_time, stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = predictor.util.align_dataframe_by_time(stocks, fed_funds_rate)\n",
    "print(stocks.shape)\n",
    "stocks = predictor.util.align_dataframe_by_time(stocks, payroll)\n",
    "print(stocks.shape)\n",
    "stocks = predictor.util.align_dataframe_by_time(stocks, cpi)\n",
    "print(stocks.shape)\n",
    "stocks = predictor.util.align_dataframe_by_time(stocks, unemployment)\n",
    "print(stocks.shape)\n",
    "stocks = predictor.util.align_dataframe_by_time(stocks, real_gdp)\n",
    "print(stocks.shape)\n",
    "stocks = predictor.util.align_dataframe_by_time(stocks, real_gdp_per_capita)\n",
    "print(stocks.shape)\n",
    "stocks = predictor.util.align_dataframe_by_time(stocks, treasury_yield)\n",
    "print(stocks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sma = ingestor.data['SMA']\n",
    "sma = sma[sma['symbol'].isin(filtered_symbols) & (sma['interval'] == \"daily\")]\n",
    "roc = ingestor.data['ROC']\n",
    "roc = roc[roc['symbol'].isin(filtered_symbols) & (roc['interval'] == \"daily\")]\n",
    "ht_sine = ingestor.data['HT_SINE']\n",
    "ht_sine = ht_sine[ht_sine['symbol'].isin(filtered_symbols) & (ht_sine['interval'] == \"daily\")]\n",
    "mom = ingestor.data['MOM']\n",
    "mom = mom[mom['symbol'].isin(filtered_symbols) & (mom['interval'] == \"daily\")]\n",
    "sma = sma.pivot(columns=['symbol', 'time_period', 'series_type'], values=['sma']).dropna(axis=1, how='all')\n",
    "sma.columns = sma.columns.map(lambda x: '-'.join([str(t) for t in x]))\n",
    "roc = roc.pivot(columns=['symbol','time_period', 'series_type'], values=['roc']).dropna(axis=1, how='all')\n",
    "roc.columns = roc.columns.map(lambda x: '-'.join([str(t) for t in x]))\n",
    "ht_sine = ht_sine.pivot(columns=['symbol','time_period', 'series_type'], values=['lead_sine', 'sine']).dropna(axis=1, how='all')\n",
    "ht_sine.columns = ht_sine.columns.map(lambda x: '-'.join([str(t) for t in x]))\n",
    "mom = mom.pivot(columns=['symbol','time_period', 'series_type'], values=['mom']).dropna(axis=1, how='all')\n",
    "mom.columns = mom.columns.map(lambda x: '-'.join([str(t) for t in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = predictor.util.align_dataframe_by_time(stocks, roc)\n",
    "print(stocks.shape)\n",
    "stocks = predictor.util.align_dataframe_by_time(stocks, ht_sine)\n",
    "print(stocks.shape)\n",
    "stocks = predictor.util.align_dataframe_by_time(stocks, mom)\n",
    "print(stocks.shape)\n",
    "stocks = predictor.util.align_dataframe_by_time(stocks, sma)\n",
    "print(stocks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(stocks, open(r'cached_data/target_stock.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = pickle.load(open(r'cached_data/target_stock.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stocks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ahead = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data = predictor.util.TimeSeriesData(endog=stocks.loc[:,stocks.columns.str.contains('_close')], exog=stocks.loc[:,~stocks.columns.str.contains('_close')], seq_len=180, label_len=n_ahead, n_ahead=n_ahead)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offline Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_params = expand_grid(\n",
    "    hidden_size=[32, 64, 128],\n",
    "    num_layers=[2, 4, 8],\n",
    "    dropout=[0.2, 0.4],\n",
    "    batch_size=[64, 128],\n",
    "    learning_rate=[0.001, 0.0001],\n",
    "    weight_decay=[0.001, 0.0001],\n",
    "    epochs=[30, 60],\n",
    "    bidirectional = [True, False],\n",
    "    n_fold=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(sweep_params.shape[0])):\n",
    "    param = sweep_params.loc[i,:].to_dict()\n",
    "    rnn = modeling.RNN(\n",
    "        input_size=stocks.shape[1],\n",
    "        n_ahead=n_ahead,\n",
    "        hidden_size=param['hidden_size'],\n",
    "        output_size=time_series_data.endog.shape[1],\n",
    "        num_layers=param['num_layers'],\n",
    "        dropout=param['dropout'],\n",
    "        bidirectional=param['bidirectional'],\n",
    "        use_gpu=True)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        rnn.parameters(),\n",
    "        lr=param['learning_rate'],\n",
    "        weight_decay=param['weight_decay'])\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    training_record = predictor.train_and_evaluate_model(\n",
    "        rnn,\n",
    "        optimizer,\n",
    "        loss_func,\n",
    "        time_series_data,\n",
    "        epochs_per_fold=param['epochs'],\n",
    "        test_ratio=0.2,\n",
    "        n_fold=param['n_fold'],\n",
    "        clip_grad=1,\n",
    "        batch_size=param['batch_size']) # Pointwise prediction\n",
    "    training_record.serialize_to_file(r'cached_data/massmodel_record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(sweep_params.shape[0])):\n",
    "    param = sweep_params.loc[i,:].to_dict()\n",
    "    rnn = modeling.LSTM(\n",
    "        input_size=stocks.shape[1],\n",
    "        n_ahead=n_ahead,\n",
    "        hidden_size=param['hidden_size'],\n",
    "        output_size=time_series_data.endog.shape[1],\n",
    "        num_layers=param['num_layers'],\n",
    "        dropout=param['dropout'],\n",
    "        bidirectional=param['bidirectional'],\n",
    "        use_gpu=True)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        rnn.parameters(),\n",
    "        lr=param['learning_rate'],\n",
    "        weight_decay=param['weight_decay'])\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    training_record = predictor.train_and_evaluate_model(\n",
    "        rnn,\n",
    "        optimizer,\n",
    "        loss_func,\n",
    "        time_series_data,\n",
    "        epochs_per_fold=param['epochs'],\n",
    "        test_ratio=0.2,\n",
    "        n_fold=param['n_fold'],\n",
    "        clip_grad=1,\n",
    "        batch_size=param['batch_size']) # Pointwise prediction\n",
    "    training_record.serialize_to_file(r'cached_data/massmodel_record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_params = expand_grid(\n",
    "    hidden_size=[32, 64, 128],\n",
    "    num_layers=[1, 2, 3, 4],\n",
    "    dropout=[0.0, 0.2, 0.4],\n",
    "    batch_size=[32, 64, 128],\n",
    "    learning_rate=[0.001, 0.005, 0.0001],\n",
    "    weight_decay=[0.001, 0.0001, 0.00001],\n",
    "    epochs=[40],\n",
    "    epochs_per_update=[1, 5, 10, 15],\n",
    "    update_freq=[1, 5, 10, 15],\n",
    "    bidirectional = [True, False]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(sweep_params.shape[0])):\n",
    "    rnn = modeling.RNN(\n",
    "        input_size=target_stock.shape[1],\n",
    "        n_ahead=n_ahead,\n",
    "        hidden_size=sweep_params.loc[i,'hidden_size'],\n",
    "        output_size=time_series_data.endog.shape[1],\n",
    "        num_layers=sweep_params.loc[i,'num_layers'],\n",
    "        dropout=sweep_params.loc[i,'dropout'],\n",
    "        bidirectional=sweep_params.loc[i,'bidirectional'],\n",
    "        use_gpu=True)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        rnn.parameters(),\n",
    "        lr=sweep_params.loc[i,'learning_rate'],\n",
    "        weight_decay=sweep_params.loc[i,'weight_decay'])\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=sweep_params.loc[i,'epochs'], epochs_per_update=sweep_params.loc[i,'epochs_per_update'], update_freq=sweep_params.loc[i,'update_freq'], clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "    training_record = predictor.train_and_evaluate_model_online(\n",
    "        rnn,\n",
    "        time_series_data,\n",
    "        update_rule,\n",
    "        optimizer,\n",
    "        loss_func,\n",
    "        train_ratio=0.3,\n",
    "        batch_size=sweep_params.loc[i,'batch_size']) # Pointwise prediction\n",
    "    training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], n_ahead=1, hidden_size=64, output_size=1, num_layers=2, dropout=0.5, bidirectional=False, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.01)\n",
    "update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=80, epochs_per_update=10, update_freq=5, clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model_online(rnn,\n",
    "                                                            time_series_data,\n",
    "                                                            update_rule,\n",
    "                                                            optimizer,\n",
    "                                                            loss_func,\n",
    "                                                            train_ratio=0.3,\n",
    "                                                            batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_record = predictor.util.ModelPerformanceOnline.deserialize_from_file(r'cached_data/record.sqlite', 2)\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_logs()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], n_ahead=1, hidden_size=64, output_size=1, num_layers=2, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=80, epochs_per_update=5, update_freq=5, clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model_online(rnn,\n",
    "                                                            time_series_data,\n",
    "                                                            update_rule,\n",
    "                                                            optimizer,\n",
    "                                                            loss_func,\n",
    "                                                            train_ratio=0.3,\n",
    "                                                            batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record = predictor.util.ModelPerformanceOnline.deserialize_from_file(r'cached_data/record.sqlite', 1)\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_logs()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], n_ahead=1, hidden_size=64, output_size=1, num_layers=3, dropout=0.2, bidirectional=False, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=80, epochs_per_update=5, update_freq=5, clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model_online(rnn,\n",
    "                                                            time_series_data,\n",
    "                                                            update_rule,\n",
    "                                                            optimizer,\n",
    "                                                            loss_func,\n",
    "                                                            train_ratio=0.3,\n",
    "                                                            batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record = predictor.util.ModelPerformanceOnline.deserialize_from_file(r'cached_data/record.sqlite', 1)\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_logs()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], n_ahead=1, hidden_size=64, output_size=1, num_layers=3, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=80, epochs_per_update=5, update_freq=5, clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model_online(rnn,\n",
    "                                                            time_series_data,\n",
    "                                                            update_rule,\n",
    "                                                            optimizer,\n",
    "                                                            loss_func,\n",
    "                                                            train_ratio=0.3,\n",
    "                                                            batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record = predictor.util.ModelPerformanceOnline.deserialize_from_file(r'cached_data/record.sqlite', 1)\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_logs()\n",
    "training_record.plot_residuals()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
