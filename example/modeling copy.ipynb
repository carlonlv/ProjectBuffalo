{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2580db88f10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r'C:\\Users\\carlo\\GitHub\\ProjectBuffalo')\n",
    "\n",
    "import buffalo.ingestion as ingestion\n",
    "import buffalo.predictor as predictor\n",
    "import buffalo.algorithm as algorithm\n",
    "import buffalo.predictor.models as modeling\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from buffalo.utility import expand_grid, do_call_for_each_group\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor = ingestion.DataIngestion(ingestion.enum.API.ADVANTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor.load_data(r'cached_data/ingestion.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stock = ingestor.data['ADJUSTED_DAILY_STOCK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_income_statement = ingestor.data['COMPANY_INCOME_STATEMENT'].query('freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function']).dropna(axis=1, how='all')\n",
    "target_balance_sheet = ingestor.data['COMPANY_BALANCE_SHEET'].query('freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function']).dropna(axis=1, how='all')\n",
    "target_cash_flow = ingestor.data['COMPANY_CASH_FLOW'].query('freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function', 'net_income']).dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_funds_rate = ingestor.data['FEDERAL_FUNDS_RATE'][['value']].rename(columns={'value': 'effective_federal_funds_rate'}).dropna(axis=1, how='all')\n",
    "payroll = ingestor.data['NONFARM_PAYROLL'][['value']].rename(columns={'value': 'total_nonfarm_payroll'}).dropna(axis=1, how='all')\n",
    "cpi = ingestor.data['CPI'][['value']].rename(columns={'value': 'consumer_price_index'}).dropna(axis=1, how='all')\n",
    "unemployment = ingestor.data['UNEMPLOYMENT'][['value']].rename(columns={'value': 'unemployment_rate'}).dropna(axis=1, how='all')\n",
    "real_gdp = ingestor.data['REAL_GDP'][['value']].rename(columns={'value': 'real_gross_domestic_product'}).dropna(axis=1, how='all')\n",
    "real_gdp_per_capita = ingestor.data['REAL_GDP_PER_CAPITA'][['value']].rename(columns={'value': 'real_gross_domestic_product_per_capita'})\n",
    "treasury_yield = ingestor.data['TREASURY_YIELD'][['value', 'maturity']].pivot(columns=['maturity'], values=['value']).dropna(axis=1, how='all')\n",
    "treasury_yield.columns = 'treasury_yield_' + treasury_yield.columns.droplevel(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_colwise_stocks(stock, all_symbols=['MSFT', 'IBM', 'JNJ', 'PFE', 'UNH', 'XLV', 'JPM', 'BAC', 'GS', 'XLF', 'AAPL', 'GE', 'KO', 'PEP', 'NKE', 'XLP', 'PG', 'HON', 'MMM', 'XLI']):\n",
    "    curr_symb = stock['symbol'].iloc[0]\n",
    "    for symb in all_symbols:\n",
    "        if symb != curr_symb:\n",
    "            temp = target_stock[target_stock['symbol'] == symb][['open', 'high', 'low', 'adjusted_close', 'volume']].rename(columns={'adjusted_close': 'close'})\n",
    "            temp.columns = symb + '_' + temp.columns\n",
    "            stock = predictor.util.align_dataframe_by_time(stock, temp)\n",
    "    return stock.drop(columns=['symbol', 'dividend_amount', 'split_coefficient', 'interval', 'adjusted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stock = do_call_for_each_group(target_stock, combine_colwise_stocks, ['symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjusted_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>MSFT_open</th>\n",
       "      <th>MSFT_high</th>\n",
       "      <th>...</th>\n",
       "      <th>XLI_open</th>\n",
       "      <th>XLI_high</th>\n",
       "      <th>XLI_low</th>\n",
       "      <th>XLI_close</th>\n",
       "      <th>XLI_volume</th>\n",
       "      <th>AAPL_open</th>\n",
       "      <th>AAPL_high</th>\n",
       "      <th>AAPL_low</th>\n",
       "      <th>AAPL_close</th>\n",
       "      <th>AAPL_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72535</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>1999-11-01 00:00:00-05:00</td>\n",
       "      <td>93.250000</td>\n",
       "      <td>94.190002</td>\n",
       "      <td>92.120003</td>\n",
       "      <td>92.370003</td>\n",
       "      <td>28.873610</td>\n",
       "      <td>26630600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>28.020000</td>\n",
       "      <td>28.020000</td>\n",
       "      <td>27.410000</td>\n",
       "      <td>17.690912</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>80.690002</td>\n",
       "      <td>77.370003</td>\n",
       "      <td>0.589041</td>\n",
       "      <td>2487300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72536</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>1999-11-02 00:00:00-05:00</td>\n",
       "      <td>92.750000</td>\n",
       "      <td>94.500000</td>\n",
       "      <td>91.940002</td>\n",
       "      <td>92.559998</td>\n",
       "      <td>28.933001</td>\n",
       "      <td>23174500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>27.690001</td>\n",
       "      <td>28.030001</td>\n",
       "      <td>27.660000</td>\n",
       "      <td>17.910355</td>\n",
       "      <td>25700.0</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>81.690002</td>\n",
       "      <td>77.309998</td>\n",
       "      <td>0.608999</td>\n",
       "      <td>3564600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72537</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>1999-11-03 00:00:00-05:00</td>\n",
       "      <td>92.940002</td>\n",
       "      <td>93.500000</td>\n",
       "      <td>91.500000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>28.757952</td>\n",
       "      <td>22258500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>27.750000</td>\n",
       "      <td>27.750000</td>\n",
       "      <td>27.559999</td>\n",
       "      <td>17.787725</td>\n",
       "      <td>8400.0</td>\n",
       "      <td>81.620003</td>\n",
       "      <td>83.250000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>0.618485</td>\n",
       "      <td>2932700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72538</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>1999-11-04 00:00:00-05:00</td>\n",
       "      <td>92.309998</td>\n",
       "      <td>92.750000</td>\n",
       "      <td>90.309998</td>\n",
       "      <td>91.750000</td>\n",
       "      <td>28.679806</td>\n",
       "      <td>27119700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>27.750000</td>\n",
       "      <td>27.910000</td>\n",
       "      <td>27.360001</td>\n",
       "      <td>17.736092</td>\n",
       "      <td>17800.0</td>\n",
       "      <td>82.059998</td>\n",
       "      <td>85.370003</td>\n",
       "      <td>80.620003</td>\n",
       "      <td>0.634574</td>\n",
       "      <td>3384700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72539</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>1999-11-05 00:00:00-05:00</td>\n",
       "      <td>91.809998</td>\n",
       "      <td>92.870003</td>\n",
       "      <td>90.500000</td>\n",
       "      <td>91.559998</td>\n",
       "      <td>28.620415</td>\n",
       "      <td>35083700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>27.629999</td>\n",
       "      <td>27.860001</td>\n",
       "      <td>27.559999</td>\n",
       "      <td>17.832905</td>\n",
       "      <td>88600.0</td>\n",
       "      <td>84.620003</td>\n",
       "      <td>88.370003</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>0.670165</td>\n",
       "      <td>3721500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78422</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2023-03-27 00:00:00-05:00</td>\n",
       "      <td>280.500000</td>\n",
       "      <td>281.458893</td>\n",
       "      <td>275.519989</td>\n",
       "      <td>276.380005</td>\n",
       "      <td>276.380005</td>\n",
       "      <td>26840212</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>97.690002</td>\n",
       "      <td>98.050003</td>\n",
       "      <td>97.080002</td>\n",
       "      <td>97.720001</td>\n",
       "      <td>7668573.0</td>\n",
       "      <td>159.940002</td>\n",
       "      <td>160.770004</td>\n",
       "      <td>157.869995</td>\n",
       "      <td>158.279999</td>\n",
       "      <td>52390266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78423</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2023-03-28 00:00:00-05:00</td>\n",
       "      <td>275.790009</td>\n",
       "      <td>276.140015</td>\n",
       "      <td>272.045105</td>\n",
       "      <td>275.230011</td>\n",
       "      <td>275.230011</td>\n",
       "      <td>21878647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>97.529999</td>\n",
       "      <td>98.610001</td>\n",
       "      <td>97.529999</td>\n",
       "      <td>98.220001</td>\n",
       "      <td>9909054.0</td>\n",
       "      <td>157.970001</td>\n",
       "      <td>158.490005</td>\n",
       "      <td>155.979996</td>\n",
       "      <td>157.649994</td>\n",
       "      <td>45992152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78424</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2023-03-29 00:00:00-05:00</td>\n",
       "      <td>278.959991</td>\n",
       "      <td>281.139801</td>\n",
       "      <td>278.410004</td>\n",
       "      <td>280.510010</td>\n",
       "      <td>280.510010</td>\n",
       "      <td>25087032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>99.110001</td>\n",
       "      <td>99.699997</td>\n",
       "      <td>98.919998</td>\n",
       "      <td>99.650002</td>\n",
       "      <td>9387900.0</td>\n",
       "      <td>159.369995</td>\n",
       "      <td>161.050003</td>\n",
       "      <td>159.350006</td>\n",
       "      <td>160.770004</td>\n",
       "      <td>51305691.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78425</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2023-03-30 00:00:00-05:00</td>\n",
       "      <td>284.230011</td>\n",
       "      <td>284.459991</td>\n",
       "      <td>281.480011</td>\n",
       "      <td>284.049988</td>\n",
       "      <td>284.049988</td>\n",
       "      <td>25053410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>100.139999</td>\n",
       "      <td>100.339996</td>\n",
       "      <td>99.540001</td>\n",
       "      <td>99.860001</td>\n",
       "      <td>8160225.0</td>\n",
       "      <td>161.529999</td>\n",
       "      <td>162.470001</td>\n",
       "      <td>161.270996</td>\n",
       "      <td>162.360001</td>\n",
       "      <td>49501689.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78426</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2023-03-31 00:00:00-05:00</td>\n",
       "      <td>283.730011</td>\n",
       "      <td>289.269989</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>288.299988</td>\n",
       "      <td>288.299988</td>\n",
       "      <td>32765976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>100.449997</td>\n",
       "      <td>101.239998</td>\n",
       "      <td>100.239998</td>\n",
       "      <td>101.180000</td>\n",
       "      <td>12006462.0</td>\n",
       "      <td>162.440002</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>161.910004</td>\n",
       "      <td>164.899994</td>\n",
       "      <td>68749792.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5892 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      symbol                      time        open        high         low  \\\n",
       "72535   MSFT 1999-11-01 00:00:00-05:00   93.250000   94.190002   92.120003   \n",
       "72536   MSFT 1999-11-02 00:00:00-05:00   92.750000   94.500000   91.940002   \n",
       "72537   MSFT 1999-11-03 00:00:00-05:00   92.940002   93.500000   91.500000   \n",
       "72538   MSFT 1999-11-04 00:00:00-05:00   92.309998   92.750000   90.309998   \n",
       "72539   MSFT 1999-11-05 00:00:00-05:00   91.809998   92.870003   90.500000   \n",
       "...      ...                       ...         ...         ...         ...   \n",
       "78422   MSFT 2023-03-27 00:00:00-05:00  280.500000  281.458893  275.519989   \n",
       "78423   MSFT 2023-03-28 00:00:00-05:00  275.790009  276.140015  272.045105   \n",
       "78424   MSFT 2023-03-29 00:00:00-05:00  278.959991  281.139801  278.410004   \n",
       "78425   MSFT 2023-03-30 00:00:00-05:00  284.230011  284.459991  281.480011   \n",
       "78426   MSFT 2023-03-31 00:00:00-05:00  283.730011  289.269989  283.000000   \n",
       "\n",
       "            close  adjusted_close    volume  MSFT_open  MSFT_high  ...  \\\n",
       "72535   92.370003       28.873610  26630600        NaN        NaN  ...   \n",
       "72536   92.559998       28.933001  23174500        NaN        NaN  ...   \n",
       "72537   92.000000       28.757952  22258500        NaN        NaN  ...   \n",
       "72538   91.750000       28.679806  27119700        NaN        NaN  ...   \n",
       "72539   91.559998       28.620415  35083700        NaN        NaN  ...   \n",
       "...           ...             ...       ...        ...        ...  ...   \n",
       "78422  276.380005      276.380005  26840212        NaN        NaN  ...   \n",
       "78423  275.230011      275.230011  21878647        NaN        NaN  ...   \n",
       "78424  280.510010      280.510010  25087032        NaN        NaN  ...   \n",
       "78425  284.049988      284.049988  25053410        NaN        NaN  ...   \n",
       "78426  288.299988      288.299988  32765976        NaN        NaN  ...   \n",
       "\n",
       "         XLI_open    XLI_high     XLI_low   XLI_close  XLI_volume   AAPL_open  \\\n",
       "72535   28.020000   28.020000   27.410000   17.690912      3100.0   80.000000   \n",
       "72536   27.690001   28.030001   27.660000   17.910355     25700.0   78.000000   \n",
       "72537   27.750000   27.750000   27.559999   17.787725      8400.0   81.620003   \n",
       "72538   27.750000   27.910000   27.360001   17.736092     17800.0   82.059998   \n",
       "72539   27.629999   27.860001   27.559999   17.832905     88600.0   84.620003   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "78422   97.690002   98.050003   97.080002   97.720001   7668573.0  159.940002   \n",
       "78423   97.529999   98.610001   97.529999   98.220001   9909054.0  157.970001   \n",
       "78424   99.110001   99.699997   98.919998   99.650002   9387900.0  159.369995   \n",
       "78425  100.139999  100.339996   99.540001   99.860001   8160225.0  161.529999   \n",
       "78426  100.449997  101.239998  100.239998  101.180000  12006462.0  162.440002   \n",
       "\n",
       "        AAPL_high    AAPL_low  AAPL_close  AAPL_volume  \n",
       "72535   80.690002   77.370003    0.589041    2487300.0  \n",
       "72536   81.690002   77.309998    0.608999    3564600.0  \n",
       "72537   83.250000   81.000000    0.618485    2932700.0  \n",
       "72538   85.370003   80.620003    0.634574    3384700.0  \n",
       "72539   88.370003   84.000000    0.670165    3721500.0  \n",
       "...           ...         ...         ...          ...  \n",
       "78422  160.770004  157.869995  158.279999   52390266.0  \n",
       "78423  158.490005  155.979996  157.649994   45992152.0  \n",
       "78424  161.050003  159.350006  160.770004   51305691.0  \n",
       "78425  162.470001  161.270996  162.360001   49501689.0  \n",
       "78426  165.000000  161.910004  164.899994   68749792.0  \n",
       "\n",
       "[5892 rows x 108 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_stock[target_stock['symbol'] == 'MSFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m target_stock \u001b[39m=\u001b[39m do_call_for_each_group(target_stock, predictor\u001b[39m.\u001b[39;49mutil\u001b[39m.\u001b[39;49malign_dataframe_by_time, [\u001b[39m'\u001b[39;49m\u001b[39msymbol\u001b[39;49m\u001b[39m'\u001b[39;49m], other_df\u001b[39m=\u001b[39;49mfed_funds_rate)\n\u001b[0;32m      2\u001b[0m payroll \u001b[39m=\u001b[39m do_call_for_each_group(target_stock, predictor\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39malign_dataframe_by_time, [\u001b[39m'\u001b[39m\u001b[39msymbol\u001b[39m\u001b[39m'\u001b[39m], other_df\u001b[39m=\u001b[39mpayroll)\n\u001b[0;32m      3\u001b[0m cpi \u001b[39m=\u001b[39m do_call_for_each_group(target_stock, predictor\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39malign_dataframe_by_time, [\u001b[39m'\u001b[39m\u001b[39msymbol\u001b[39m\u001b[39m'\u001b[39m], other_df\u001b[39m=\u001b[39mcpi)\n",
      "File \u001b[1;32m~\\GitHub\\ProjectBuffalo\\buffalo\\utility\\__init__.py:90\u001b[0m, in \u001b[0;36mdo_call_for_each_group\u001b[1;34m(data, func, grouping, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(data, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 90\u001b[0m     \u001b[39mreturn\u001b[39;00m data\u001b[39m.\u001b[39;49mgroupby(grouping)\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: do_call(func, x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\u001b[39m.\u001b[39mreset_index()\n",
      "File \u001b[1;32mc:\\Users\\carlo\\miniconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1549\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1547\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1548\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1549\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_apply_general(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selected_obj)\n\u001b[0;32m   1550\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   1551\u001b[0m         \u001b[39m# gh-20949\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m         \u001b[39m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1556\u001b[0m         \u001b[39m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m         \u001b[39m# on a string grouper column\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_group_selection_context():\n",
      "File \u001b[1;32mc:\\Users\\carlo\\miniconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1601\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m   1565\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_python_apply_general\u001b[39m(\n\u001b[0;32m   1566\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1571\u001b[0m     is_agg: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1572\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDFrameT:\n\u001b[0;32m   1573\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m \u001b[39m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1575\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1599\u001b[0m \u001b[39m        data after applying f\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1601\u001b[0m     values, mutated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouper\u001b[39m.\u001b[39;49mapply(f, data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxis)\n\u001b[0;32m   1602\u001b[0m     \u001b[39mif\u001b[39;00m not_indexed_same \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1603\u001b[0m         not_indexed_same \u001b[39m=\u001b[39m mutated \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmutated\n",
      "File \u001b[1;32mc:\\Users\\carlo\\miniconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:839\u001b[0m, in \u001b[0;36mBaseGrouper.apply\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39m# group might be modified\u001b[39;00m\n\u001b[0;32m    838\u001b[0m group_axes \u001b[39m=\u001b[39m group\u001b[39m.\u001b[39maxes\n\u001b[1;32m--> 839\u001b[0m res \u001b[39m=\u001b[39m f(group)\n\u001b[0;32m    840\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mutated \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    841\u001b[0m     mutated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\GitHub\\ProjectBuffalo\\buffalo\\utility\\__init__.py:90\u001b[0m, in \u001b[0;36mdo_call_for_each_group.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(data, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 90\u001b[0m     \u001b[39mreturn\u001b[39;00m data\u001b[39m.\u001b[39mgroupby(grouping)\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: do_call(func, x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\u001b[39m.\u001b[39mreset_index()\n",
      "File \u001b[1;32m~\\GitHub\\ProjectBuffalo\\buffalo\\utility\\__init__.py:75\u001b[0m, in \u001b[0;36mdo_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m sig \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(func)\n\u001b[0;32m     74\u001b[0m filtered_dict \u001b[39m=\u001b[39m {filter_item[\u001b[39m0\u001b[39m] : filter_item[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m filter_item \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m filter_item[\u001b[39m0\u001b[39m] \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(sig\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mkeys())[\u001b[39mlen\u001b[39m(args):] \u001b[39mand\u001b[39;00m filter_item[\u001b[39m1\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m}\n\u001b[1;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfiltered_dict)\n",
      "File \u001b[1;32m~\\GitHub\\ProjectBuffalo\\buffalo\\predictor\\util.py:92\u001b[0m, in \u001b[0;36malign_dataframe_by_time\u001b[1;34m(target_df, other_df, max_period)\u001b[0m\n\u001b[0;32m     90\u001b[0m other_df \u001b[39m=\u001b[39m other_df\u001b[39m.\u001b[39msort_index()\n\u001b[0;32m     91\u001b[0m target_df \u001b[39m=\u001b[39m target_df\u001b[39m.\u001b[39msort_index()\n\u001b[1;32m---> 92\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(target_df\u001b[39m.\u001b[39mindex, pd\u001b[39m.\u001b[39mDatetimeIndex) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(other_df\u001b[39m.\u001b[39mindex, pd\u001b[39m.\u001b[39mDatetimeIndex)\n\u001b[0;32m     93\u001b[0m expire_time \u001b[39m=\u001b[39m predict_next_timestamps(other_df\u001b[39m.\u001b[39mindex, max_period\u001b[39m=\u001b[39mmax_period)\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m other_df\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mtz \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "target_stock = do_call_for_each_group(target_stock, predictor.util.align_dataframe_by_time, ['symbol'], other_df=fed_funds_rate)\n",
    "payroll = do_call_for_each_group(target_stock, predictor.util.align_dataframe_by_time, ['symbol'], other_df=payroll)\n",
    "cpi = do_call_for_each_group(target_stock, predictor.util.align_dataframe_by_time, ['symbol'], other_df=cpi)\n",
    "unemployment = do_call_for_each_group(target_stock, predictor.util.align_dataframe_by_time, ['symbol'], other_df=unemployment)\n",
    "real_gdp = do_call_for_each_group(target_stock, predictor.util.align_dataframe_by_time, ['symbol'], other_df=real_gdp)\n",
    "real_gdp_per_capita = do_call_for_each_group(target_stock, predictor.util.align_dataframe_by_time, ['symbol'], other_df=real_gdp_per_capita)\n",
    "treasury_yield = do_call_for_each_group(target_stock, predictor.util.align_dataframe_by_time, ['symbol'], other_df=treasury_yield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_colwise_indicators(stock, indicators):\n",
    "    curr_symb = stock['symbol'].iloc[0]\n",
    "    for ind in indicators:\n",
    "        stock = predictor.util.align_dataframe_by_time(stock, ind.query(f'symbol == \"{curr_symb}\"').drop(columns=['symbol']))\n",
    "    return stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sma = ingestor.data['SMA'].query('interval == \"daily\"')\n",
    "roc = ingestor.data['ROC'].query('interval == \"daily\"')\n",
    "ht_sine = ingestor.data['HT_SINE'].query('interval == \"daily\"')\n",
    "mom = ingestor.data['MOM'].query('interval == \"daily\"')\n",
    "sma = sma.pivot(columns=['time_period', 'series_type'], values=['sma']).dropna(axis=1, how='all')\n",
    "sma.columns = sma.columns.map(lambda x: '-'.join([str(t) for t in x]))\n",
    "roc = roc.pivot(columns=['time_period', 'series_type'], values=['roc']).dropna(axis=1, how='all')\n",
    "roc.columns = roc.columns.map(lambda x: '-'.join([str(t) for t in x]))\n",
    "ht_sine = ht_sine.pivot(columns=['time_period', 'series_type'], values=['lead_sine', 'sine']).dropna(axis=1, how='all')\n",
    "ht_sine.columns = ht_sine.columns.map(lambda x: '-'.join([str(t) for t in x]))\n",
    "mom = mom.pivot(columns=['time_period', 'series_type'], values=['mom']).dropna(axis=1, how='all')\n",
    "mom.columns = mom.columns.map(lambda x: '-'.join([str(t) for t in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treasury_yield = do_call_for_each_group(target_stock, combine_colwise_indicators, ['symbol'], indicators=[sma, roc, ht_sine, mom])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(target_stock, open(r'cached_data/target_stock.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stock = pickle.load(open(r'cached_data/target_stock.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stock.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_head = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data = predictor.util.TimeSeriesData(endog=target_stock[['adjusted_close']], exog=target_stock.drop(columns=['adjusted_close']), seq_len=180, label_len=n_head, name=f'DAILY_ADJUSTED_CLOSE_{target_symbol}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offline Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_params = expand_grid(\n",
    "    hidden_size=[32, 64, 128],\n",
    "    num_layers=[1, 2, 4],\n",
    "    dropout=[0.0, 0.2, 0.4],\n",
    "    batch_size=[32, 64, 128],\n",
    "    learning_rate=[0.001, 0.005, 0.0001],\n",
    "    weight_decay=[0.001, 0.0001],\n",
    "    epochs=[20, 30],\n",
    "    bidirectional = [True, False],\n",
    "    n_fold=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(6, sweep_params.shape[0])):\n",
    "    param = sweep_params.loc[i,:].to_dict()\n",
    "    rnn = modeling.RNN(\n",
    "        input_size=target_stock.shape[1],\n",
    "        n_ahead=n_head,\n",
    "        hidden_size=param['hidden_size'],\n",
    "        output_size=1,\n",
    "        num_layers=param['num_layers'],\n",
    "        dropout=param['dropout'],\n",
    "        bidirectional=param['bidirectional'],\n",
    "        use_gpu=True)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        rnn.parameters(),\n",
    "        lr=param['learning_rate'],\n",
    "        weight_decay=param['weight_decay'])\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    training_record = predictor.train_and_evaluate_model(\n",
    "        rnn,\n",
    "        optimizer,\n",
    "        loss_func,\n",
    "        time_series_data,\n",
    "        epochs_per_fold=param['epochs'],\n",
    "        test_ratio=0.2,\n",
    "        n_fold=param['n_fold'],\n",
    "        clip_grad=1,\n",
    "        batch_size=param['batch_size']) # Pointwise prediction\n",
    "    training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(sweep_params.shape[0])):\n",
    "    param = sweep_params.loc[i,:].to_dict()\n",
    "    rnn = modeling.LSTM(\n",
    "        input_size=target_stock.shape[1],\n",
    "        n_ahead=n_head,\n",
    "        hidden_size=param['hidden_size'],\n",
    "        output_size=1,\n",
    "        num_layers=param['num_layers'],\n",
    "        dropout=param['dropout'],\n",
    "        bidirectional=param['bidirectional'],\n",
    "        use_gpu=True)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        rnn.parameters(),\n",
    "        lr=param['learning_rate'],\n",
    "        weight_decay=param['weight_decay'])\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    training_record = predictor.train_and_evaluate_model(\n",
    "        rnn,\n",
    "        optimizer,\n",
    "        loss_func,\n",
    "        time_series_data,\n",
    "        epochs_per_fold=param['epochs'],\n",
    "        test_ratio=0.2,\n",
    "        n_fold=param['n_fold'],\n",
    "        clip_grad=1,\n",
    "        batch_size=param['batch_size']) # Pointwise prediction\n",
    "    training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_params = expand_grid(\n",
    "    hidden_size=[32, 64, 128],\n",
    "    num_layers=[1, 2, 3, 4],\n",
    "    dropout=[0.0, 0.2, 0.4],\n",
    "    batch_size=[32, 64, 128],\n",
    "    learning_rate=[0.001, 0.005, 0.0001],\n",
    "    weight_decay=[0.001, 0.0001, 0.00001],\n",
    "    epochs=[40],\n",
    "    epochs_per_update=[1, 5, 10, 15],\n",
    "    update_freq=[1, 5, 10, 15],\n",
    "    bidirectional = [True, False]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(sweep_params.shape[0])):\n",
    "    rnn = modeling.RNN(\n",
    "        input_size=target_stock.shape[1],\n",
    "        n_ahead=n_head,\n",
    "        hidden_size=sweep_params.loc[i,'hidden_size'],\n",
    "        output_size=1,\n",
    "        num_layers=sweep_params.loc[i,'num_layers'],\n",
    "        dropout=sweep_params.loc[i,'dropout'],\n",
    "        bidirectional=sweep_params.loc[i,'bidirectional'],\n",
    "        use_gpu=True)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        rnn.parameters(),\n",
    "        lr=sweep_params.loc[i,'learning_rate'],\n",
    "        weight_decay=sweep_params.loc[i,'weight_decay'])\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=sweep_params.loc[i,'epochs'], epochs_per_update=sweep_params.loc[i,'epochs_per_update'], update_freq=sweep_params.loc[i,'update_freq'], clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "    training_record = predictor.train_and_evaluate_model_online(\n",
    "        rnn,\n",
    "        time_series_data,\n",
    "        update_rule,\n",
    "        optimizer,\n",
    "        loss_func,\n",
    "        train_ratio=0.3,\n",
    "        batch_size=sweep_params.loc[i,'batch_size']) # Pointwise prediction\n",
    "    training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], n_ahead=1, hidden_size=64, output_size=1, num_layers=2, dropout=0.5, bidirectional=False, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.01)\n",
    "update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=80, epochs_per_update=10, update_freq=5, clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model_online(rnn,\n",
    "                                                            time_series_data,\n",
    "                                                            update_rule,\n",
    "                                                            optimizer,\n",
    "                                                            loss_func,\n",
    "                                                            train_ratio=0.3,\n",
    "                                                            batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_record = predictor.util.ModelPerformanceOnline.deserialize_from_file(r'cached_data/record.sqlite', 2)\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_logs()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], n_ahead=1, hidden_size=64, output_size=1, num_layers=2, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=80, epochs_per_update=5, update_freq=5, clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model_online(rnn,\n",
    "                                                            time_series_data,\n",
    "                                                            update_rule,\n",
    "                                                            optimizer,\n",
    "                                                            loss_func,\n",
    "                                                            train_ratio=0.3,\n",
    "                                                            batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record = predictor.util.ModelPerformanceOnline.deserialize_from_file(r'cached_data/record.sqlite', 1)\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_logs()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], n_ahead=1, hidden_size=64, output_size=1, num_layers=3, dropout=0.2, bidirectional=False, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=80, epochs_per_update=5, update_freq=5, clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model_online(rnn,\n",
    "                                                            time_series_data,\n",
    "                                                            update_rule,\n",
    "                                                            optimizer,\n",
    "                                                            loss_func,\n",
    "                                                            train_ratio=0.3,\n",
    "                                                            batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record = predictor.util.ModelPerformanceOnline.deserialize_from_file(r'cached_data/record.sqlite', 1)\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_logs()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], n_ahead=1, hidden_size=64, output_size=1, num_layers=3, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=80, epochs_per_update=5, update_freq=5, clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model_online(rnn,\n",
    "                                                            time_series_data,\n",
    "                                                            update_rule,\n",
    "                                                            optimizer,\n",
    "                                                            loss_func,\n",
    "                                                            train_ratio=0.3,\n",
    "                                                            batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record = predictor.util.ModelPerformanceOnline.deserialize_from_file(r'cached_data/record.sqlite', 1)\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_logs()\n",
    "training_record.plot_residuals()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
