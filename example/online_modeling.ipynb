{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x231637c0ed0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r'C:\\Users\\carlo\\GitHub\\ProjectBuffalo')\n",
    "\n",
    "import buffalo.ingestion as ingestion\n",
    "import buffalo.predictor as predictor\n",
    "import buffalo.algorithm as algorithm\n",
    "import buffalo.predictor.models as modeling\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from buffalo.utility import expand_grid\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor = ingestion.DataIngestion(ingestion.enum.API.ADVANTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor.load_data(r'cached_data/ingestion.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_symbol = 'GE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AAPL', 'MSFT', 'IBM', 'META', 'JNJ', 'PFE', 'UNH', 'MARK', 'XLV',\n",
       "       'JPM', 'BAC', 'GS', 'MS', 'XLF', 'PG', 'KO', 'PEP', 'NKE', 'XLP',\n",
       "       'GE', 'HON', 'UTX', 'MMM', 'XLI'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestor.data['ADJUSTED_DAILY_STOCK'].symbol.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stock = ingestor.data['ADJUSTED_DAILY_STOCK'].query('symbol == @target_symbol')[['open', 'high', 'low', 'adjusted_close', 'volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_stocks = ingestor.data['ADJUSTED_DAILY_STOCK'].query('symbol != @target_symbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjusted_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>dividend_amount</th>\n",
       "      <th>split_coefficient</th>\n",
       "      <th>symbol</th>\n",
       "      <th>interval</th>\n",
       "      <th>adjusted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-03-31 00:00:00-05:00</th>\n",
       "      <td>162.440002</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>161.910004</td>\n",
       "      <td>164.899994</td>\n",
       "      <td>164.899994</td>\n",
       "      <td>68749792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>daily</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-30 00:00:00-05:00</th>\n",
       "      <td>161.529999</td>\n",
       "      <td>162.470001</td>\n",
       "      <td>161.270996</td>\n",
       "      <td>162.360001</td>\n",
       "      <td>162.360001</td>\n",
       "      <td>49501689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>daily</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-29 00:00:00-05:00</th>\n",
       "      <td>159.369995</td>\n",
       "      <td>161.050003</td>\n",
       "      <td>159.350006</td>\n",
       "      <td>160.770004</td>\n",
       "      <td>160.770004</td>\n",
       "      <td>51305691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>daily</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-28 00:00:00-05:00</th>\n",
       "      <td>157.970001</td>\n",
       "      <td>158.490005</td>\n",
       "      <td>155.979996</td>\n",
       "      <td>157.649994</td>\n",
       "      <td>157.649994</td>\n",
       "      <td>45992152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>daily</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-27 00:00:00-05:00</th>\n",
       "      <td>159.940002</td>\n",
       "      <td>160.770004</td>\n",
       "      <td>157.869995</td>\n",
       "      <td>158.279999</td>\n",
       "      <td>158.279999</td>\n",
       "      <td>52390266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>daily</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-05 00:00:00-05:00</th>\n",
       "      <td>27.629999</td>\n",
       "      <td>27.860001</td>\n",
       "      <td>27.559999</td>\n",
       "      <td>27.629999</td>\n",
       "      <td>17.832905</td>\n",
       "      <td>88600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>XLI</td>\n",
       "      <td>daily</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-04 00:00:00-05:00</th>\n",
       "      <td>27.750000</td>\n",
       "      <td>27.910000</td>\n",
       "      <td>27.360001</td>\n",
       "      <td>27.480000</td>\n",
       "      <td>17.736092</td>\n",
       "      <td>17800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>XLI</td>\n",
       "      <td>daily</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-03 00:00:00-05:00</th>\n",
       "      <td>27.750000</td>\n",
       "      <td>27.750000</td>\n",
       "      <td>27.559999</td>\n",
       "      <td>27.559999</td>\n",
       "      <td>17.787725</td>\n",
       "      <td>8400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>XLI</td>\n",
       "      <td>daily</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-02 00:00:00-05:00</th>\n",
       "      <td>27.690001</td>\n",
       "      <td>28.030001</td>\n",
       "      <td>27.660000</td>\n",
       "      <td>27.750000</td>\n",
       "      <td>17.910355</td>\n",
       "      <td>25700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>XLI</td>\n",
       "      <td>daily</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-01 00:00:00-05:00</th>\n",
       "      <td>28.020000</td>\n",
       "      <td>28.020000</td>\n",
       "      <td>27.410000</td>\n",
       "      <td>27.410000</td>\n",
       "      <td>17.690912</td>\n",
       "      <td>3100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>XLI</td>\n",
       "      <td>daily</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131455 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 open        high         low       close  \\\n",
       "time                                                                        \n",
       "2023-03-31 00:00:00-05:00  162.440002  165.000000  161.910004  164.899994   \n",
       "2023-03-30 00:00:00-05:00  161.529999  162.470001  161.270996  162.360001   \n",
       "2023-03-29 00:00:00-05:00  159.369995  161.050003  159.350006  160.770004   \n",
       "2023-03-28 00:00:00-05:00  157.970001  158.490005  155.979996  157.649994   \n",
       "2023-03-27 00:00:00-05:00  159.940002  160.770004  157.869995  158.279999   \n",
       "...                               ...         ...         ...         ...   \n",
       "1999-11-05 00:00:00-05:00   27.629999   27.860001   27.559999   27.629999   \n",
       "1999-11-04 00:00:00-05:00   27.750000   27.910000   27.360001   27.480000   \n",
       "1999-11-03 00:00:00-05:00   27.750000   27.750000   27.559999   27.559999   \n",
       "1999-11-02 00:00:00-05:00   27.690001   28.030001   27.660000   27.750000   \n",
       "1999-11-01 00:00:00-05:00   28.020000   28.020000   27.410000   27.410000   \n",
       "\n",
       "                           adjusted_close    volume  dividend_amount  \\\n",
       "time                                                                   \n",
       "2023-03-31 00:00:00-05:00      164.899994  68749792              0.0   \n",
       "2023-03-30 00:00:00-05:00      162.360001  49501689              0.0   \n",
       "2023-03-29 00:00:00-05:00      160.770004  51305691              0.0   \n",
       "2023-03-28 00:00:00-05:00      157.649994  45992152              0.0   \n",
       "2023-03-27 00:00:00-05:00      158.279999  52390266              0.0   \n",
       "...                                   ...       ...              ...   \n",
       "1999-11-05 00:00:00-05:00       17.832905     88600              0.0   \n",
       "1999-11-04 00:00:00-05:00       17.736092     17800              0.0   \n",
       "1999-11-03 00:00:00-05:00       17.787725      8400              0.0   \n",
       "1999-11-02 00:00:00-05:00       17.910355     25700              0.0   \n",
       "1999-11-01 00:00:00-05:00       17.690912      3100              0.0   \n",
       "\n",
       "                           split_coefficient symbol interval  adjusted  \n",
       "time                                                                    \n",
       "2023-03-31 00:00:00-05:00                1.0   AAPL    daily         1  \n",
       "2023-03-30 00:00:00-05:00                1.0   AAPL    daily         1  \n",
       "2023-03-29 00:00:00-05:00                1.0   AAPL    daily         1  \n",
       "2023-03-28 00:00:00-05:00                1.0   AAPL    daily         1  \n",
       "2023-03-27 00:00:00-05:00                1.0   AAPL    daily         1  \n",
       "...                                      ...    ...      ...       ...  \n",
       "1999-11-05 00:00:00-05:00                1.0    XLI    daily         1  \n",
       "1999-11-04 00:00:00-05:00                1.0    XLI    daily         1  \n",
       "1999-11-03 00:00:00-05:00                1.0    XLI    daily         1  \n",
       "1999-11-02 00:00:00-05:00                1.0    XLI    daily         1  \n",
       "1999-11-01 00:00:00-05:00                1.0    XLI    daily         1  \n",
       "\n",
       "[131455 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol in ['MSFT', 'IBM', 'JNJ', 'PFE', 'UNH', 'XLV', 'JPM', 'BAC', 'GS', 'XLF', 'AAPL', 'KO', 'PEP', 'NKE', 'XLP', 'PG', 'HON', 'MMM', 'XLI']:\n",
    "    temp = other_stocks[other_stocks['symbol'] == symbol][['open', 'high', 'low', 'adjusted_close', 'volume']].rename(columns={'adjusted_close': 'close'})\n",
    "    temp.columns = symbol + '_' + temp.columns\n",
    "    target_stock = predictor.util.align_dataframe_by_time(target_stock, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_income_statement = ingestor.data['COMPANY_INCOME_STATEMENT'].query('symbol == @target_symbol & freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function']).dropna(axis=1, how='all')\n",
    "target_balance_sheet = ingestor.data['COMPANY_BALANCE_SHEET'].query('symbol == @target_symbol & freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function']).dropna(axis=1, how='all')\n",
    "target_cash_flow = ingestor.data['COMPANY_CASH_FLOW'].query('symbol == @target_symbol & freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function', 'net_income']).dropna(axis=1, how='all')\n",
    "fed_funds_rate = ingestor.data['FEDERAL_FUNDS_RATE'][['value']].rename(columns={'value': 'effective_federal_funds_rate'}).dropna(axis=1, how='all')\n",
    "payroll = ingestor.data['NONFARM_PAYROLL'][['value']].rename(columns={'value': 'total_nonfarm_payroll'}).dropna(axis=1, how='all')\n",
    "cpi = ingestor.data['CPI'][['value']].rename(columns={'value': 'consumer_price_index'}).dropna(axis=1, how='all')\n",
    "unemployment = ingestor.data['UNEMPLOYMENT'][['value']].rename(columns={'value': 'unemployment_rate'}).dropna(axis=1, how='all')\n",
    "real_gdp = ingestor.data['REAL_GDP'][['value']].rename(columns={'value': 'real_gross_domestic_product'}).dropna(axis=1, how='all')\n",
    "real_gdp_per_capita = ingestor.data['REAL_GDP_PER_CAPITA'][['value']].rename(columns={'value': 'real_gross_domestic_product_per_capita'})\n",
    "treasury_yield = ingestor.data['TREASURY_YIELD'][['value', 'maturity']].pivot(columns=['maturity'], values=['value']).dropna(axis=1, how='all')\n",
    "treasury_yield.columns = 'treasury_yield_' + treasury_yield.columns.droplevel(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sma = ingestor.data['SMA'].query('symbol == @target_symbol & interval == \"daily\"')\n",
    "roc = ingestor.data['ROC'].query('symbol == @target_symbol & interval == \"daily\"')\n",
    "ht_sine = ingestor.data['HT_SINE'].query('symbol == @target_symbol & interval == \"daily\"')\n",
    "mom = ingestor.data['MOM'].query('symbol == @target_symbol & interval == \"daily\"')\n",
    "sma = sma.pivot(columns=['time_period', 'series_type'], values=['sma']).dropna(axis=1, how='all')\n",
    "sma.columns = sma.columns.map(lambda x: '-'.join([str(t) for t in x]))\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, sma)\n",
    "roc = roc.pivot(columns=['time_period', 'series_type'], values=['roc']).dropna(axis=1, how='all')\n",
    "roc.columns = roc.columns.map(lambda x: '-'.join([str(t) for t in x]))\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, roc)\n",
    "ht_sine = ht_sine.pivot(columns=['time_period', 'series_type'], values=['lead_sine', 'sine']).dropna(axis=1, how='all')\n",
    "ht_sine.columns = ht_sine.columns.map(lambda x: '-'.join([str(t) for t in x]))\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, ht_sine)\n",
    "mom = mom.pivot(columns=['time_period', 'series_type'], values=['mom']).dropna(axis=1, how='all')\n",
    "mom.columns = mom.columns.map(lambda x: '-'.join([str(t) for t in x]))\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5692, 160)\n",
      "(5691, 161)\n",
      "(5671, 162)\n",
      "(5671, 163)\n",
      "(5671, 164)\n",
      "(5286, 165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carlo\\miniconda3\\lib\\site-packages\\scipy\\signal\\_spectral_py.py:1999: UserWarning: nperseg = 256 is greater than input length  = 83, using nperseg = 83\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5286, 166)\n",
      "(5246, 171)\n"
     ]
    }
   ],
   "source": [
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, fed_funds_rate)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, payroll)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, cpi)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, unemployment)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, real_gdp)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, real_gdp_per_capita)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, treasury_yield)\n",
    "print(target_stock.shape)\n",
    "#target_stock = predictor.util.align_dataframe_by_time(target_stock, target_income_statement)\n",
    "#print(target_stock.shape)\n",
    "#target_stock = predictor.util.align_dataframe_by_time(target_stock, target_balance_sheet)\n",
    "#print(target_stock.shape)\n",
    "#target_stock = predictor.util.align_dataframe_by_time(target_stock, target_cash_flow)\n",
    "#print(target_stock.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(target_stock, open(r'cached_data/target_stock.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stock = pickle.load(open(r'cached_data/target_stock.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5246, 171)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_stock.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_head = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data = predictor.util.TimeSeriesData(endog=target_stock[['adjusted_close']], exog=target_stock.drop(columns=['adjusted_close']), seq_len=180, label_len=n_head, name=f'DAILY_ADJUSTED_CLOSE_{target_symbol}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offline Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_params = expand_grid(\n",
    "    hidden_size=[32, 64, 128],\n",
    "    num_layers=[1, 2, 3, 4],\n",
    "    dropout=[0.0, 0.2, 0.4],\n",
    "    batch_size=[32, 64, 128],\n",
    "    learning_rate=[0.001, 0.005, 0.0001],\n",
    "    weight_decay=[0.001, 0.0001, 0.00001],\n",
    "    epochs=[40],\n",
    "    epochs_per_update=[1, 5, 10, 15],\n",
    "    update_freq=[1, 5, 10, 15],\n",
    "    bidirectional = [True, False]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9242a6d8282c4abe8cf5064dc0782f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a68ce1b7424fae86149948c0786dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Online training and testing.:   0%|          | 0/3545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "TimeSeriesData.serialize_to_file() got an unexpected keyword argument 'additional_note_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 26\u001b[0m\n\u001b[0;32m     17\u001b[0m update_rule \u001b[39m=\u001b[39m algorithm\u001b[39m.\u001b[39monline_update\u001b[39m.\u001b[39mIncrementalBatchGradientDescent(epochs\u001b[39m=\u001b[39msweep_params\u001b[39m.\u001b[39mloc[i,\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m], epochs_per_update\u001b[39m=\u001b[39msweep_params\u001b[39m.\u001b[39mloc[i,\u001b[39m'\u001b[39m\u001b[39mepochs_per_update\u001b[39m\u001b[39m'\u001b[39m], update_freq\u001b[39m=\u001b[39msweep_params\u001b[39m.\u001b[39mloc[i,\u001b[39m'\u001b[39m\u001b[39mupdate_freq\u001b[39m\u001b[39m'\u001b[39m], clip_grad_norm_update\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, clip_grad_norm_train\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m training_record \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39mtrain_and_evaluate_model_online(\n\u001b[0;32m     19\u001b[0m     rnn,\n\u001b[0;32m     20\u001b[0m     time_series_data,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     train_ratio\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m,\n\u001b[0;32m     25\u001b[0m     batch_size\u001b[39m=\u001b[39mparam[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m# Pointwise prediction\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m training_record\u001b[39m.\u001b[39;49mserialize_to_file(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcached_data/online_record.sqlite\u001b[39;49m\u001b[39m'\u001b[39;49m, additional_note_dataset\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, additonal_note_model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32m~\\GitHub\\ProjectBuffalo\\buffalo\\predictor\\util.py:604\u001b[0m, in \u001b[0;36mModelPerformanceOnline.serialize_to_file\u001b[1;34m(self, sql_path, additional_note_dataset, additonal_note_model)\u001b[0m\n\u001b[0;32m    601\u001b[0m newconn \u001b[39m=\u001b[39m sqlite3\u001b[39m.\u001b[39mconnect(sql_path)\n\u001b[0;32m    603\u001b[0m \u001b[39m## Store Dataset Information\u001b[39;00m\n\u001b[1;32m--> 604\u001b[0m searched_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49mserialize_to_file(sql_path, additional_note_dataset\u001b[39m=\u001b[39;49madditional_note_dataset, newconn\u001b[39m=\u001b[39;49mnewconn)\n\u001b[0;32m    605\u001b[0m dataset_id \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(searched_id)\n\u001b[0;32m    607\u001b[0m \u001b[39m## Store Model Information\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: TimeSeriesData.serialize_to_file() got an unexpected keyword argument 'additional_note_dataset'"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(sweep_params.shape[0])):\n",
    "    param = sweep_params.loc[i,:].to_dict()\n",
    "    rnn = modeling.RNN(\n",
    "        input_size=target_stock.shape[1],\n",
    "        n_ahead=n_head,\n",
    "        hidden_size=param['hidden_size'],\n",
    "        output_size=1,\n",
    "        num_layers=param['num_layers'],\n",
    "        dropout=param['dropout'],\n",
    "        bidirectional=param['bidirectional'],\n",
    "        use_gpu=True)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        rnn.parameters(),\n",
    "        lr=param['learning_rate'],\n",
    "        weight_decay=param['weight_decay'])\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=sweep_params.loc[i,'epochs'], epochs_per_update=sweep_params.loc[i,'epochs_per_update'], update_freq=sweep_params.loc[i,'update_freq'], clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "    training_record = predictor.train_and_evaluate_model_online(\n",
    "        rnn,\n",
    "        time_series_data,\n",
    "        update_rule,\n",
    "        optimizer,\n",
    "        loss_func,\n",
    "        train_ratio=0.3,\n",
    "        batch_size=param['batch_size']) # Pointwise prediction\n",
    "    training_record.serialize_to_file(r'cached_data/online_record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(sweep_params.shape[0])):\n",
    "    param = sweep_params.loc[i,:].to_dict()\n",
    "    rnn = modeling.LSTM(\n",
    "        input_size=target_stock.shape[1],\n",
    "        n_ahead=n_head,\n",
    "        hidden_size=param['hidden_size'],\n",
    "        output_size=1,\n",
    "        num_layers=param['num_layers'],\n",
    "        dropout=param['dropout'],\n",
    "        bidirectional=param['bidirectional'],\n",
    "        use_gpu=True)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        rnn.parameters(),\n",
    "        lr=param['learning_rate'],\n",
    "        weight_decay=param['weight_decay'])\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=sweep_params.loc[i,'epochs'], epochs_per_update=sweep_params.loc[i,'epochs_per_update'], update_freq=sweep_params.loc[i,'update_freq'], clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "    training_record = predictor.train_and_evaluate_model_online(\n",
    "        rnn,\n",
    "        time_series_data,\n",
    "        update_rule,\n",
    "        optimizer,\n",
    "        loss_func,\n",
    "        train_ratio=0.3,\n",
    "        batch_size=param['batch_size']) # Pointwise prediction\n",
    "    training_record.serialize_to_file(r'cached_data/online_record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_record = predictor.util.ModelPerformanceOnline.deserialize_from_file(r'cached_data/record.sqlite', 2)\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_logs()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], n_ahead=1, hidden_size=64, output_size=1, num_layers=2, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=80, epochs_per_update=5, update_freq=5, clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model_online(rnn,\n",
    "                                                            time_series_data,\n",
    "                                                            update_rule,\n",
    "                                                            optimizer,\n",
    "                                                            loss_func,\n",
    "                                                            train_ratio=0.3,\n",
    "                                                            batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record = predictor.util.ModelPerformanceOnline.deserialize_from_file(r'cached_data/record.sqlite', 1)\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_logs()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], n_ahead=1, hidden_size=64, output_size=1, num_layers=3, dropout=0.2, bidirectional=False, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=80, epochs_per_update=5, update_freq=5, clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model_online(rnn,\n",
    "                                                            time_series_data,\n",
    "                                                            update_rule,\n",
    "                                                            optimizer,\n",
    "                                                            loss_func,\n",
    "                                                            train_ratio=0.3,\n",
    "                                                            batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record = predictor.util.ModelPerformanceOnline.deserialize_from_file(r'cached_data/record.sqlite', 1)\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_logs()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], n_ahead=1, hidden_size=64, output_size=1, num_layers=3, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=80, epochs_per_update=5, update_freq=5, clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model_online(rnn,\n",
    "                                                            time_series_data,\n",
    "                                                            update_rule,\n",
    "                                                            optimizer,\n",
    "                                                            loss_func,\n",
    "                                                            train_ratio=0.3,\n",
    "                                                            batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record = predictor.util.ModelPerformanceOnline.deserialize_from_file(r'cached_data/record.sqlite', 1)\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_logs()\n",
    "training_record.plot_residuals()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
