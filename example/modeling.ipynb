{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r'C:\\Users\\carlo\\GitHub\\ProjectBuffalo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import buffalo.ingestion as ingestion\n",
    "import buffalo.predictor as predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor = ingestion.DataIngestion(ingestion.enum.API.ADVANTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor.load_data(r'cached_data/ingestion.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_symbol = 'AAPL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor.data['ADJUSTED_DAILY_STOCK'].symbol.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stock = ingestor.data['ADJUSTED_DAILY_STOCK'].query('symbol == @target_symbol')[['open', 'high', 'low', 'adjusted_close', 'volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_stocks = ingestor.data['ADJUSTED_DAILY_STOCK'].query('symbol != \"AAPL\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol in ['MSFT', 'IBM', 'JNJ', 'PFE', 'UNH', 'XLV', 'JPM', 'BAC', 'GS', 'XLF', 'PG', 'KO', 'PEP', 'NKE', 'XLP', 'GE', 'HON', 'MMM', 'XLI']:\n",
    "    temp = other_stocks[other_stocks['symbol'] == symbol][['open', 'high', 'low', 'adjusted_close', 'volume']].rename(columns={'adjusted_close': 'close'})\n",
    "    temp.columns = symbol + '_' + temp.columns\n",
    "    target_stock = predictor.util.align_dataframe_by_time(target_stock, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_income_statement = ingestor.data['COMPANY_INCOME_STATEMENT'].query('symbol == @target_symbol & freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function']).dropna(axis=1, how='all')\n",
    "target_balance_sheet = ingestor.data['COMPANY_BALANCE_SHEET'].query('symbol == @target_symbol & freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function']).dropna(axis=1, how='all')\n",
    "target_cash_flow = ingestor.data['COMPANY_CASH_FLOW'].query('symbol == @target_symbol & freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function', 'net_income']).dropna(axis=1, how='all')\n",
    "fed_funds_rate = ingestor.data['FEDERAL_FUNDS_RATE'][['value']].rename(columns={'value': 'effective_federal_funds_rate'}).dropna(axis=1, how='all')\n",
    "payroll = ingestor.data['NONFARM_PAYROLL'][['value']].rename(columns={'value': 'total_nonfarm_payroll'}).dropna(axis=1, how='all')\n",
    "cpi = ingestor.data['CPI'][['value']].rename(columns={'value': 'consumer_price_index'}).dropna(axis=1, how='all')\n",
    "unemployment = ingestor.data['UNEMPLOYMENT'][['value']].rename(columns={'value': 'unemployment_rate'}).dropna(axis=1, how='all')\n",
    "real_gdp = ingestor.data['REAL_GDP'][['value']].rename(columns={'value': 'real_gross_domestic_product'}).dropna(axis=1, how='all')\n",
    "real_gdp_per_capita = ingestor.data['REAL_GDP_PER_CAPITA'][['value']].rename(columns={'value': 'real_gross_domestic_product_per_capita'})\n",
    "treasury_yield = ingestor.data['TREASURY_YIELD'][['value', 'maturity']].pivot(columns=['maturity'], values=['value']).dropna(axis=1, how='all')\n",
    "treasury_yield.columns = 'treasury_yield_' + treasury_yield.columns.droplevel(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, fed_funds_rate)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, payroll)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, cpi)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, unemployment)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, real_gdp)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, real_gdp_per_capita)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, treasury_yield)\n",
    "print(target_stock.shape)\n",
    "#target_stock = predictor.util.align_dataframe_by_time(target_stock, target_income_statement)\n",
    "#print(target_stock.shape)\n",
    "#target_stock = predictor.util.align_dataframe_by_time(target_stock, target_balance_sheet)\n",
    "#print(target_stock.shape)\n",
    "#target_stock = predictor.util.align_dataframe_by_time(target_stock, target_cash_flow)\n",
    "#print(target_stock.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(target_stock, open(r'cached_data/target_stock.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stock = pickle.load(open('cached_data/target_stock.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data = predictor.util.TimeSeriesData(endog=target_stock[['adjusted_close']], exog=target_stock.drop(columns=['adjusted_close']), seq_len=180, name='DAILY_ADJUSTED_CLOSE_APPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import buffalo.predictor.models as modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=2, dropout=0.2, bidirectional=False, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(rnn, optimizer, loss_func, time_series_data, 30, 0.3, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=2, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(rnn, optimizer, loss_func, time_series_data, 30, 0.3, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=3, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(rnn, optimizer, loss_func, time_series_data, 30, 0.3, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=4, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(rnn, optimizer, loss_func, time_series_data, 30, 0.3, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = modeling.LSTM(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=2, dropout=0.2, bidirectional=False, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(lstm, optimizer, loss_func, time_series_data, 30, 0.3, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = modeling.LSTM(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=2, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(lstm, optimizer, loss_func, time_series_data, 30, 0.3, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = modeling.LSTM(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=3, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(lstm, optimizer, loss_func, time_series_data, 30, 0.3, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = modeling.LSTM(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=4, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(lstm, optimizer, loss_func, time_series_data, 30, 0.3, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = modeling.LSTM(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=5, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(lstm, optimizer, loss_func, time_series_data, 30, 0.3, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
