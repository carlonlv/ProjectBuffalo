{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r'C:\\Users\\carlo\\GitHub\\ProjectBuffalo')\n",
    "\n",
    "import buffalo.ingestion as ingestion\n",
    "import buffalo.predictor as predictor\n",
    "import buffalo.algorithm as algorithm\n",
    "import buffalo.predictor.models as modeling\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor = ingestion.DataIngestion(ingestion.enum.API.ADVANTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor.load_data(r'cached_data/ingestion.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_symbol = 'GE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AAPL', 'MSFT', 'IBM', 'META', 'JNJ', 'PFE', 'UNH', 'MARK', 'XLV',\n",
       "       'JPM', 'BAC', 'GS', 'MS', 'XLF', 'PG', 'KO', 'PEP', 'NKE', 'XLP',\n",
       "       'GE', 'HON', 'UTX', 'MMM', 'XLI'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestor.data['ADJUSTED_DAILY_STOCK'].symbol.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stock = ingestor.data['ADJUSTED_DAILY_STOCK'].query('symbol == @target_symbol')[['open', 'high', 'low', 'adjusted_close', 'volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_stocks = ingestor.data['ADJUSTED_DAILY_STOCK'].query('symbol != @target_symbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Indexes have overlapping values: Index(['AAPL_open', 'AAPL_high', 'AAPL_low', 'AAPL_close', 'AAPL_volume'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m temp \u001b[39m=\u001b[39m other_stocks[other_stocks[\u001b[39m'\u001b[39m\u001b[39msymbol\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m symbol][[\u001b[39m'\u001b[39m\u001b[39mopen\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhigh\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlow\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39madjusted_close\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvolume\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39madjusted_close\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mclose\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m      3\u001b[0m temp\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m symbol \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m temp\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m----> 4\u001b[0m target_stock \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39;49mutil\u001b[39m.\u001b[39;49malign_dataframe_by_time(target_stock, temp)\n",
      "File \u001b[1;32m~\\GitHub\\ProjectBuffalo\\buffalo\\predictor\\util.py:103\u001b[0m, in \u001b[0;36malign_dataframe_by_time\u001b[1;34m(target_df, other_df, max_period)\u001b[0m\n\u001b[0;32m    101\u001b[0m other_df \u001b[39m=\u001b[39m other_df\u001b[39m.\u001b[39mreindex(target_df\u001b[39m.\u001b[39mindex)\n\u001b[0;32m    102\u001b[0m other_df \u001b[39m=\u001b[39m other_df[other_df\u001b[39m.\u001b[39mindex \u001b[39m<\u001b[39m expire_time\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> 103\u001b[0m result \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat([target_df, other_df], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, verify_integrity\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    104\u001b[0m result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mdropna()\n\u001b[0;32m    105\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\carlo\\miniconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\carlo\\miniconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[0;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    158\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m    159\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39m    1   3   4\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    369\u001b[0m         objs,\n\u001b[0;32m    370\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    371\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[0;32m    372\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[0;32m    373\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[0;32m    374\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[0;32m    375\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m    376\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[0;32m    377\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    378\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    379\u001b[0m     )\n\u001b[0;32m    381\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\carlo\\miniconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:563\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverify_integrity \u001b[39m=\u001b[39m verify_integrity\n\u001b[0;32m    561\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39m=\u001b[39m copy\n\u001b[1;32m--> 563\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_axes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_new_axes()\n",
      "File \u001b[1;32mc:\\Users\\carlo\\miniconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:633\u001b[0m, in \u001b[0;36m_Concatenator._get_new_axes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_new_axes\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Index]:\n\u001b[0;32m    632\u001b[0m     ndim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_result_dim()\n\u001b[1;32m--> 633\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_concat_axis \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbm_axis \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_comb_axis(i)\n\u001b[0;32m    635\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ndim)\n\u001b[0;32m    636\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\carlo\\miniconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:634\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_new_axes\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Index]:\n\u001b[0;32m    632\u001b[0m     ndim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_result_dim()\n\u001b[0;32m    633\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m--> 634\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concat_axis \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbm_axis \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_comb_axis(i)\n\u001b[0;32m    635\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ndim)\n\u001b[0;32m    636\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\carlo\\miniconda3\\lib\\site-packages\\pandas\\_libs\\properties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\carlo\\miniconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:697\u001b[0m, in \u001b[0;36m_Concatenator._get_concat_axis\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    693\u001b[0m     concat_axis \u001b[39m=\u001b[39m _make_concat_multiindex(\n\u001b[0;32m    694\u001b[0m         indexes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlevels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames\n\u001b[0;32m    695\u001b[0m     )\n\u001b[1;32m--> 697\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_check_integrity(concat_axis)\n\u001b[0;32m    699\u001b[0m \u001b[39mreturn\u001b[39;00m concat_axis\n",
      "File \u001b[1;32mc:\\Users\\carlo\\miniconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:705\u001b[0m, in \u001b[0;36m_Concatenator._maybe_check_integrity\u001b[1;34m(self, concat_index)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m concat_index\u001b[39m.\u001b[39mis_unique:\n\u001b[0;32m    704\u001b[0m     overlap \u001b[39m=\u001b[39m concat_index[concat_index\u001b[39m.\u001b[39mduplicated()]\u001b[39m.\u001b[39munique()\n\u001b[1;32m--> 705\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIndexes have overlapping values: \u001b[39m\u001b[39m{\u001b[39;00moverlap\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Indexes have overlapping values: Index(['AAPL_open', 'AAPL_high', 'AAPL_low', 'AAPL_close', 'AAPL_volume'], dtype='object')"
     ]
    }
   ],
   "source": [
    "for symbol in ['MSFT', 'IBM', 'JNJ', 'PFE', 'UNH', 'XLV', 'JPM', 'BAC', 'GS', 'XLF', 'AAPL', 'KO', 'PEP', 'NKE', 'XLP', 'PG', 'HON', 'MMM', 'XLI']:\n",
    "    temp = other_stocks[other_stocks['symbol'] == symbol][['open', 'high', 'low', 'adjusted_close', 'volume']].rename(columns={'adjusted_close': 'close'})\n",
    "    temp.columns = symbol + '_' + temp.columns\n",
    "    target_stock = predictor.util.align_dataframe_by_time(target_stock, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_income_statement = ingestor.data['COMPANY_INCOME_STATEMENT'].query('symbol == @target_symbol & freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function']).dropna(axis=1, how='all')\n",
    "target_balance_sheet = ingestor.data['COMPANY_BALANCE_SHEET'].query('symbol == @target_symbol & freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function']).dropna(axis=1, how='all')\n",
    "target_cash_flow = ingestor.data['COMPANY_CASH_FLOW'].query('symbol == @target_symbol & freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function', 'net_income']).dropna(axis=1, how='all')\n",
    "fed_funds_rate = ingestor.data['FEDERAL_FUNDS_RATE'][['value']].rename(columns={'value': 'effective_federal_funds_rate'}).dropna(axis=1, how='all')\n",
    "payroll = ingestor.data['NONFARM_PAYROLL'][['value']].rename(columns={'value': 'total_nonfarm_payroll'}).dropna(axis=1, how='all')\n",
    "cpi = ingestor.data['CPI'][['value']].rename(columns={'value': 'consumer_price_index'}).dropna(axis=1, how='all')\n",
    "unemployment = ingestor.data['UNEMPLOYMENT'][['value']].rename(columns={'value': 'unemployment_rate'}).dropna(axis=1, how='all')\n",
    "real_gdp = ingestor.data['REAL_GDP'][['value']].rename(columns={'value': 'real_gross_domestic_product'}).dropna(axis=1, how='all')\n",
    "real_gdp_per_capita = ingestor.data['REAL_GDP_PER_CAPITA'][['value']].rename(columns={'value': 'real_gross_domestic_product_per_capita'})\n",
    "treasury_yield = ingestor.data['TREASURY_YIELD'][['value', 'maturity']].pivot(columns=['maturity'], values=['value']).dropna(axis=1, how='all')\n",
    "treasury_yield.columns = 'treasury_yield_' + treasury_yield.columns.droplevel(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sma = ingestor.data['SMA'].query('symbol == @target_symbol & interval == \"daily\"')\n",
    "roc = ingestor.data['ROC'].query('symbol == @target_symbol & interval == \"daily\"')\n",
    "ht_sine = ingestor.data['HT_SINE'].query('symbol == @target_symbol & interval == \"daily\"')\n",
    "mom = ingestor.data['MOM'].query('symbol == @target_symbol & interval == \"daily\"')\n",
    "sma = sma.pivot(columns=['time_period', 'series_type'], values=['sma']).dropna(axis=1, how='all')\n",
    "sma.columns = sma.columns.map(lambda x: '-'.join([str(t) for t in x]))\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, sma)\n",
    "roc = roc.pivot(columns=['time_period', 'series_type'], values=['roc']).dropna(axis=1, how='all')\n",
    "roc.columns = roc.columns.map(lambda x: '-'.join([str(t) for t in x]))\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, roc)\n",
    "ht_sine = ht_sine.pivot(columns=['time_period', 'series_type'], values=['lead_sine', 'sine']).dropna(axis=1, how='all')\n",
    "ht_sine.columns = ht_sine.columns.map(lambda x: '-'.join([str(t) for t in x]))\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, ht_sine)\n",
    "mom = mom.pivot(columns=['time_period', 'series_type'], values=['mom']).dropna(axis=1, how='all')\n",
    "mom.columns = mom.columns.map(lambda x: '-'.join([str(t) for t in x]))\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, fed_funds_rate)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, payroll)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, cpi)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, unemployment)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, real_gdp)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, real_gdp_per_capita)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, treasury_yield)\n",
    "print(target_stock.shape)\n",
    "#target_stock = predictor.util.align_dataframe_by_time(target_stock, target_income_statement)\n",
    "#print(target_stock.shape)\n",
    "#target_stock = predictor.util.align_dataframe_by_time(target_stock, target_balance_sheet)\n",
    "#print(target_stock.shape)\n",
    "#target_stock = predictor.util.align_dataframe_by_time(target_stock, target_cash_flow)\n",
    "#print(target_stock.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(target_stock, open(r'cached_data/target_stock.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stock = pickle.load(open('cached_data/target_stock.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data = predictor.util.TimeSeriesData(endog=target_stock[['adjusted_close']], exog=target_stock.drop(columns=['adjusted_close']), seq_len=180, label_len=1, name=f'DAILY_ADJUSTED_CLOSE_{target_symbol}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], n_ahead=1, hidden_size=64, output_size=1, num_layers=2, dropout=0.2, bidirectional=False, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model(rnn, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=2, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model(rnn, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=3, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model(rnn, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=4, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model(rnn, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=5, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model(rnn, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = modeling.LSTM(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=2, dropout=0.2, bidirectional=False, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model(lstm, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = modeling.LSTM(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=2, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model(lstm, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = modeling.LSTM(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=3, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model(lstm, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = modeling.LSTM(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=4, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model(lstm, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = modeling.LSTM(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=5, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model(lstm, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], n_ahead=1, hidden_size=64, output_size=1, num_layers=2, dropout=0.2, bidirectional=False, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=30, epochs_per_update=5, update_freq=5, clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model_online(rnn,\n",
    "                                                            time_series_data,\n",
    "                                                            update_rule,\n",
    "                                                            optimizer,\n",
    "                                                            loss_func,\n",
    "                                                            train_ratio=0.3,\n",
    "                                                            batch_size=64) # Pointwise prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_record.plot_training_records()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_record.plot_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_record = predictor.util.ModelPerformanceOnline.deserialize_from_file(r'cached_data/record.sqlite', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_record.plot_residuals()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
