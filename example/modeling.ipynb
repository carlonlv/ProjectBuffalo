{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r'C:\\Users\\carlo\\GitHub\\ProjectBuffalo')\n",
    "\n",
    "import buffalo.ingestion as ingestion\n",
    "import buffalo.predictor as predictor\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor = ingestion.DataIngestion(ingestion.enum.API.ADVANTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor.load_data(r'cached_data/ingestion.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_symbol = 'MSFT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AAPL', 'MSFT', 'IBM', 'META', 'JNJ', 'PFE', 'UNH', 'MARK', 'XLV',\n",
       "       'JPM', 'BAC', 'GS', 'MS', 'XLF', 'PG', 'KO', 'PEP', 'NKE', 'XLP',\n",
       "       'GE', 'HON', 'UTX', 'MMM', 'XLI'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestor.data['ADJUSTED_DAILY_STOCK'].symbol.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stock = ingestor.data['ADJUSTED_DAILY_STOCK'].query('symbol == @target_symbol')[['open', 'high', 'low', 'adjusted_close', 'volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_stocks = ingestor.data['ADJUSTED_DAILY_STOCK'].query('symbol != @target_symbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m temp \u001b[39m=\u001b[39m other_stocks[other_stocks[\u001b[39m'\u001b[39m\u001b[39msymbol\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m symbol][[\u001b[39m'\u001b[39m\u001b[39mopen\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhigh\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlow\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39madjusted_close\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvolume\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39madjusted_close\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mclose\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m      3\u001b[0m temp\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m symbol \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m temp\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m----> 4\u001b[0m target_stock \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39;49mutil\u001b[39m.\u001b[39;49malign_dataframe_by_time(target_stock, temp)\n",
      "File \u001b[1;32m~\\GitHub\\ProjectBuffalo\\buffalo\\predictor\\util.py:88\u001b[0m, in \u001b[0;36malign_dataframe_by_time\u001b[1;34m(target_df, other_df, max_period)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[39mAlign the other dataframe to the first dataframe. by index, where indices are assumed to be of type Timestamp.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39m:return: The concatenated dataframe by rows.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(target_df\u001b[39m.\u001b[39mindex, pd\u001b[39m.\u001b[39mDatetimeIndex) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(other_df\u001b[39m.\u001b[39mindex, pd\u001b[39m.\u001b[39mDatetimeIndex)\n\u001b[1;32m---> 88\u001b[0m expire_time \u001b[39m=\u001b[39m predict_next_timestamps(other_df\u001b[39m.\u001b[39;49mindex, max_period\u001b[39m=\u001b[39;49mmax_period)\n\u001b[0;32m     89\u001b[0m \u001b[39mif\u001b[39;00m other_df\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mtz \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     other_df\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m other_df\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mtz_localize(target_df\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mtz)\n",
      "File \u001b[1;32m~\\GitHub\\ProjectBuffalo\\buffalo\\predictor\\util.py:43\u001b[0m, in \u001b[0;36mpredict_next_timestamps\u001b[1;34m(timestamps, num_indices, max_period)\u001b[0m\n\u001b[0;32m     41\u001b[0m     warn(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMultiple resolutions in timestamps detected, using smallest time unit \u001b[39m\u001b[39m{\u001b[39;00munit\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 43\u001b[0m     unit \u001b[39m=\u001b[39m units[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m unit \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mD\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     45\u001b[0m     tmp[\u001b[39m'\u001b[39m\u001b[39mts\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39m86400\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "for symbol in ['AAPL', 'IBM', 'JNJ', 'PFE', 'UNH', 'XLV', 'JPM', 'BAC', 'GS', 'XLF', 'PG', 'KO', 'PEP', 'NKE', 'XLP', 'GE', 'HON', 'MMM', 'XLI']:\n",
    "    temp = other_stocks[other_stocks['symbol'] == symbol][['open', 'high', 'low', 'adjusted_close', 'volume']].rename(columns={'adjusted_close': 'close'})\n",
    "    temp.columns = symbol + '_' + temp.columns\n",
    "    target_stock = predictor.util.align_dataframe_by_time(target_stock, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_income_statement = ingestor.data['COMPANY_INCOME_STATEMENT'].query('symbol == @target_symbol & freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function']).dropna(axis=1, how='all')\n",
    "target_balance_sheet = ingestor.data['COMPANY_BALANCE_SHEET'].query('symbol == @target_symbol & freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function']).dropna(axis=1, how='all')\n",
    "target_cash_flow = ingestor.data['COMPANY_CASH_FLOW'].query('symbol == @target_symbol & freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function', 'net_income']).dropna(axis=1, how='all')\n",
    "fed_funds_rate = ingestor.data['FEDERAL_FUNDS_RATE'][['value']].rename(columns={'value': 'effective_federal_funds_rate'}).dropna(axis=1, how='all')\n",
    "payroll = ingestor.data['NONFARM_PAYROLL'][['value']].rename(columns={'value': 'total_nonfarm_payroll'}).dropna(axis=1, how='all')\n",
    "cpi = ingestor.data['CPI'][['value']].rename(columns={'value': 'consumer_price_index'}).dropna(axis=1, how='all')\n",
    "unemployment = ingestor.data['UNEMPLOYMENT'][['value']].rename(columns={'value': 'unemployment_rate'}).dropna(axis=1, how='all')\n",
    "real_gdp = ingestor.data['REAL_GDP'][['value']].rename(columns={'value': 'real_gross_domestic_product'}).dropna(axis=1, how='all')\n",
    "real_gdp_per_capita = ingestor.data['REAL_GDP_PER_CAPITA'][['value']].rename(columns={'value': 'real_gross_domestic_product_per_capita'})\n",
    "treasury_yield = ingestor.data['TREASURY_YIELD'][['value', 'maturity']].pivot(columns=['maturity'], values=['value']).dropna(axis=1, how='all')\n",
    "treasury_yield.columns = 'treasury_yield_' + treasury_yield.columns.droplevel(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, fed_funds_rate)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, payroll)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, cpi)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, unemployment)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, real_gdp)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, real_gdp_per_capita)\n",
    "print(target_stock.shape)\n",
    "target_stock = predictor.util.align_dataframe_by_time(target_stock, treasury_yield)\n",
    "print(target_stock.shape)\n",
    "#target_stock = predictor.util.align_dataframe_by_time(target_stock, target_income_statement)\n",
    "#print(target_stock.shape)\n",
    "#target_stock = predictor.util.align_dataframe_by_time(target_stock, target_balance_sheet)\n",
    "#print(target_stock.shape)\n",
    "#target_stock = predictor.util.align_dataframe_by_time(target_stock, target_cash_flow)\n",
    "#print(target_stock.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(target_stock, open(r'cached_data/target_stock.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stock = pickle.load(open('cached_data/target_stock.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data = predictor.util.TimeSeriesData(endog=target_stock[['adjusted_close']], exog=target_stock.drop(columns=['adjusted_close']), seq_len=180, name='DAILY_ADJUSTED_CLOSE_APPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import buffalo.predictor.models as modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=2, dropout=0.2, bidirectional=False, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(rnn, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=2, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(rnn, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=3, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(rnn, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=4, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(rnn, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=5, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(rnn, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = modeling.LSTM(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=2, dropout=0.2, bidirectional=False, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(lstm, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = modeling.LSTM(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=2, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(lstm, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = modeling.LSTM(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=3, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(lstm, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = modeling.LSTM(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=4, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(lstm, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = modeling.LSTM(input_size=target_stock.shape[1], hidden_size=64, output_size=1, num_layers=5, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = modeling.train_and_evaluate_model(lstm, optimizer, loss_func, time_series_data, 30, 0.2, 5, clip_grad=1, batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_residuals()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
