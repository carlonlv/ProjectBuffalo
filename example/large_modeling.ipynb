{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ee6b8ecef0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r'C:\\Users\\carlo\\GitHub\\ProjectBuffalo')\n",
    "\n",
    "import buffalo.ingestion as ingestion\n",
    "import buffalo.predictor as predictor\n",
    "import buffalo.algorithm as algorithm\n",
    "import buffalo.predictor.models as modeling\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from buffalo.utility import expand_grid\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor = ingestion.DataIngestion(ingestion.enum.API.ADVANTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor.load_data(r'cached_data/ingestion.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stock = ingestor.data['ADJUSTED_DAILY_STOCK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_income_statement = ingestor.data['COMPANY_INCOME_STATEMENT'].query('freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function']).dropna(axis=1, how='all')\n",
    "target_balance_sheet = ingestor.data['COMPANY_BALANCE_SHEET'].query('freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function']).dropna(axis=1, how='all')\n",
    "target_cash_flow = ingestor.data['COMPANY_CASH_FLOW'].query('freq == \"quarterly\"').drop(columns=['reported_currency', 'symbol', 'freq', 'function', 'net_income']).dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_funds_rate = ingestor.data['FEDERAL_FUNDS_RATE'][['value']].rename(columns={'value': 'effective_federal_funds_rate'}).dropna(axis=1, how='all')\n",
    "payroll = ingestor.data['NONFARM_PAYROLL'][['value']].rename(columns={'value': 'total_nonfarm_payroll'}).dropna(axis=1, how='all')\n",
    "cpi = ingestor.data['CPI'][['value']].rename(columns={'value': 'consumer_price_index'}).dropna(axis=1, how='all')\n",
    "unemployment = ingestor.data['UNEMPLOYMENT'][['value']].rename(columns={'value': 'unemployment_rate'}).dropna(axis=1, how='all')\n",
    "real_gdp = ingestor.data['REAL_GDP'][['value']].rename(columns={'value': 'real_gross_domestic_product'}).dropna(axis=1, how='all')\n",
    "real_gdp_per_capita = ingestor.data['REAL_GDP_PER_CAPITA'][['value']].rename(columns={'value': 'real_gross_domestic_product_per_capita'})\n",
    "treasury_yield = ingestor.data['TREASURY_YIELD'][['value', 'maturity']].pivot(columns=['maturity'], values=['value']).dropna(axis=1, how='all')\n",
    "treasury_yield.columns = 'treasury_yield_' + treasury_yield.columns.droplevel(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sma = ingestor.data['SMA']\n",
    "sma = sma[sma['interval'] == \"daily\"]\n",
    "roc = ingestor.data['ROC']\n",
    "roc = roc[roc['interval'] == \"daily\"]\n",
    "ht_sine = ingestor.data['HT_SINE']\n",
    "ht_sine = ht_sine[ht_sine['interval'] == \"daily\"]\n",
    "mom = ingestor.data['MOM']\n",
    "mom = mom[mom['interval'] == \"daily\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ahead = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each symbol, build a dataset\n",
    "dataset = []\n",
    "for sym, stock_df in tqdm(target_stock.groupby('symbol')):\n",
    "    stock_df = stock_df.drop(columns=['close', 'dividend_amount', 'split_coefficient', 'symbol', 'interval'])\n",
    "    stock_df = predictor.util.align_dataframe_by_time(stock_df, fed_funds_rate)\n",
    "    stock_df = predictor.util.align_dataframe_by_time(stock_df, payroll)\n",
    "    stock_df = predictor.util.align_dataframe_by_time(stock_df, cpi)\n",
    "    stock_df = predictor.util.align_dataframe_by_time(stock_df, unemployment)\n",
    "    stock_df = predictor.util.align_dataframe_by_time(stock_df, real_gdp)\n",
    "    stock_df = predictor.util.align_dataframe_by_time(stock_df, real_gdp_per_capita)\n",
    "    stock_df = predictor.util.align_dataframe_by_time(stock_df, treasury_yield)\n",
    "    curr_sma = sma.query(f'symbol == \"{sym}\"').drop(columns=['symbol', 'function', 'interval'])\n",
    "    curr_sma = curr_sma.pivot(columns=['time_period', 'series_type'], values=['sma']).dropna(axis=1, how='all')\n",
    "    curr_sma.columns = curr_sma.columns.map(lambda x: '-'.join([str(t) for t in x]))\n",
    "    stock_df = predictor.util.align_dataframe_by_time(stock_df, curr_sma)\n",
    "    curr_roc = roc.query(f'symbol == \"{sym}\"').drop(columns=['symbol', 'function', 'interval'])\n",
    "    curr_roc = curr_roc.pivot(columns=['time_period', 'series_type'], values=['roc']).dropna(axis=1, how='all')\n",
    "    curr_roc.columns = curr_roc.columns.map(lambda x: '-'.join([str(t) for t in x]))\n",
    "    stock_df = predictor.util.align_dataframe_by_time(stock_df, curr_roc)\n",
    "    curr_ht_sine = ht_sine.query(f'symbol == \"{sym}\"').drop(columns=['symbol', 'function', 'interval'])\n",
    "    curr_ht_sine = curr_ht_sine.pivot(columns=['time_period', 'series_type'], values=['lead_sine', 'sine']).dropna(axis=1, how='all')\n",
    "    curr_ht_sine.columns = curr_ht_sine.columns.map(lambda x: '-'.join([str(t) for t in x]))\n",
    "    stock_df = predictor.util.align_dataframe_by_time(stock_df, curr_ht_sine)\n",
    "    curr_mom = mom.query(f'symbol == \"{sym}\"').drop(columns=['symbol', 'function', 'interval'])\n",
    "    curr_mom = curr_mom.pivot(columns=['time_period', 'series_type'], values=['mom']).dropna(axis=1, how='all')\n",
    "    curr_mom.columns = curr_mom.columns.map(lambda x: '-'.join([str(t) for t in x]))\n",
    "    stock_df = predictor.util.align_dataframe_by_time(stock_df, curr_mom)\n",
    "    time_series_data = predictor.util.TimeSeriesData(endog=stock_df[['adjusted_close']], exog=stock_df.drop(columns=['adjusted_close']), seq_len=180, label_len=n_ahead, n_ahead=n_ahead, name=f'DAILY_ADJUSTED_CLOSE_{sym}')\n",
    "    dataset.append(time_series_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dataset, open(r'cached_data/target_stock.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle.load(open(r'cached_data/target_stock.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data = predictor.util.TimeSeriesDataCollection(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offline Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_params = expand_grid(\n",
    "    batch_size=64,\n",
    "    n_fold=1,\n",
    "    learning_rate=[0.0001, 0.001],\n",
    "    weight_decay=[0.001, 0.0001],\n",
    "    dropout=[0.2, 0.4],\n",
    "    epochs=[120],\n",
    "    bidirectional = [True, False],\n",
    "    hidden_size=[32, 64, 128],\n",
    "    num_layers=[4, 8]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(sweep_params.shape[0])):\n",
    "    param = sweep_params.loc[i,:].to_dict()\n",
    "    rnn = modeling.RNN(\n",
    "        input_size=77,\n",
    "        n_ahead=n_ahead,\n",
    "        hidden_size=param['hidden_size'],\n",
    "        output_size=1,\n",
    "        num_layers=param['num_layers'],\n",
    "        dropout=param['dropout'],\n",
    "        bidirectional=param['bidirectional'],\n",
    "        use_gpu=True)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        rnn.parameters(),\n",
    "        lr=param['learning_rate'],\n",
    "        weight_decay=param['weight_decay'])\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    training_record = predictor.train_and_evaluate_model(\n",
    "        rnn,\n",
    "        optimizer,\n",
    "        loss_func,\n",
    "        time_series_data,\n",
    "        epochs=param['epochs'],\n",
    "        test_ratio=0.2,\n",
    "        n_fold=param['n_fold'],\n",
    "        clip_grad=1,\n",
    "        batch_size=param['batch_size']) # Pointwise prediction\n",
    "    training_record.serialize_to_file(r'cached_data/massmodel_record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(sweep_params.shape[0])):\n",
    "    param = sweep_params.loc[i,:].to_dict()\n",
    "    rnn = modeling.LSTM(\n",
    "        input_size=77,\n",
    "        n_ahead=n_ahead,\n",
    "        hidden_size=param['hidden_size'],\n",
    "        output_size=1,\n",
    "        num_layers=param['num_layers'],\n",
    "        dropout=param['dropout'],\n",
    "        bidirectional=param['bidirectional'],\n",
    "        use_gpu=True)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        rnn.parameters(),\n",
    "        lr=param['learning_rate'],\n",
    "        weight_decay=param['weight_decay'])\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    training_record = predictor.train_and_evaluate_model(\n",
    "        rnn,\n",
    "        optimizer,\n",
    "        loss_func,\n",
    "        time_series_data,\n",
    "        epochs=param['epochs'],\n",
    "        test_ratio=0.2,\n",
    "        n_fold=param['n_fold'],\n",
    "        clip_grad=1,\n",
    "        batch_size=param['batch_size']) # Pointwise prediction\n",
    "    training_record.serialize_to_file(r'cached_data/massmodel_record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_params = expand_grid(\n",
    "    hidden_size=[32, 64, 128],\n",
    "    num_layers=[1, 2, 3, 4],\n",
    "    dropout=[0.0, 0.2, 0.4],\n",
    "    batch_size=[32, 64, 128],\n",
    "    learning_rate=[0.001, 0.005, 0.0001],\n",
    "    weight_decay=[0.001, 0.0001, 0.00001],\n",
    "    epochs=[40],\n",
    "    epochs_per_update=[1, 5, 10, 15],\n",
    "    update_freq=[1, 5, 10, 15],\n",
    "    bidirectional = [True, False]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(sweep_params.shape[0])):\n",
    "    rnn = modeling.RNN(\n",
    "        input_size=target_stock.shape[1],\n",
    "        n_ahead=n_ahead,\n",
    "        hidden_size=sweep_params.loc[i,'hidden_size'],\n",
    "        output_size=time_series_data.endog.shape[1],\n",
    "        num_layers=sweep_params.loc[i,'num_layers'],\n",
    "        dropout=sweep_params.loc[i,'dropout'],\n",
    "        bidirectional=sweep_params.loc[i,'bidirectional'],\n",
    "        use_gpu=True)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        rnn.parameters(),\n",
    "        lr=sweep_params.loc[i,'learning_rate'],\n",
    "        weight_decay=sweep_params.loc[i,'weight_decay'])\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=sweep_params.loc[i,'epochs'], epochs_per_update=sweep_params.loc[i,'epochs_per_update'], update_freq=sweep_params.loc[i,'update_freq'], clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "    training_record = predictor.train_and_evaluate_model_online(\n",
    "        rnn,\n",
    "        time_series_data,\n",
    "        update_rule,\n",
    "        optimizer,\n",
    "        loss_func,\n",
    "        train_ratio=0.3,\n",
    "        batch_size=sweep_params.loc[i,'batch_size']) # Pointwise prediction\n",
    "    training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], n_ahead=1, hidden_size=64, output_size=1, num_layers=2, dropout=0.5, bidirectional=False, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.01)\n",
    "update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=80, epochs_per_update=10, update_freq=5, clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model_online(rnn,\n",
    "                                                            time_series_data,\n",
    "                                                            update_rule,\n",
    "                                                            optimizer,\n",
    "                                                            loss_func,\n",
    "                                                            train_ratio=0.3,\n",
    "                                                            batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_record = predictor.util.ModelPerformanceOnline.deserialize_from_file(r'cached_data/record.sqlite', 2)\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_logs()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], n_ahead=1, hidden_size=64, output_size=1, num_layers=2, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=80, epochs_per_update=5, update_freq=5, clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model_online(rnn,\n",
    "                                                            time_series_data,\n",
    "                                                            update_rule,\n",
    "                                                            optimizer,\n",
    "                                                            loss_func,\n",
    "                                                            train_ratio=0.3,\n",
    "                                                            batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record = predictor.util.ModelPerformanceOnline.deserialize_from_file(r'cached_data/record.sqlite', 1)\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_logs()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], n_ahead=1, hidden_size=64, output_size=1, num_layers=3, dropout=0.2, bidirectional=False, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=80, epochs_per_update=5, update_freq=5, clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model_online(rnn,\n",
    "                                                            time_series_data,\n",
    "                                                            update_rule,\n",
    "                                                            optimizer,\n",
    "                                                            loss_func,\n",
    "                                                            train_ratio=0.3,\n",
    "                                                            batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record = predictor.util.ModelPerformanceOnline.deserialize_from_file(r'cached_data/record.sqlite', 1)\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_logs()\n",
    "training_record.plot_residuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = modeling.RNN(input_size=target_stock.shape[1], n_ahead=1, hidden_size=64, output_size=1, num_layers=3, dropout=0.2, bidirectional=True, use_gpu=True)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "update_rule = algorithm.online_update.IncrementalBatchGradientDescent(epochs=80, epochs_per_update=5, update_freq=5, clip_grad_norm_update=None, clip_grad_norm_train=1)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "training_record = predictor.train_and_evaluate_model_online(rnn,\n",
    "                                                            time_series_data,\n",
    "                                                            update_rule,\n",
    "                                                            optimizer,\n",
    "                                                            loss_func,\n",
    "                                                            train_ratio=0.3,\n",
    "                                                            batch_size=64) # Pointwise prediction\n",
    "training_record.serialize_to_file(r'cached_data/record.sqlite', additional_note_dataset='', additonal_note_model='')\n",
    "training_record = predictor.util.ModelPerformanceOnline.deserialize_from_file(r'cached_data/record.sqlite', 1)\n",
    "training_record.plot_training_records()\n",
    "training_record.plot_logs()\n",
    "training_record.plot_residuals()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
